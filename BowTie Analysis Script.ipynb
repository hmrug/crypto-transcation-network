{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from multiprocessing import Process, Queue\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm inspired by https://github.com/jeroenvldj/bow-tie_detection/blob/master/bow-tie_detection.py\n",
    "\n",
    "def bowtie_analysis(G):\n",
    "    # reverse all direction of the graph\n",
    "    GT = nx.reverse(G, copy=True)\n",
    "    # calculate SSC\n",
    "    SSC = max(list(nx.strongly_connected_components(G)),key=len)    \n",
    "    \n",
    "    \n",
    "    # take any node n from SSC and do a depth first search \n",
    "    # through directed graph beginning from node n\n",
    "    v_any = list(SSC)[0]\n",
    "    DFS_G = set(nx.dfs_tree(G,v_any).nodes())\n",
    "    DFS_GT = set(nx.dfs_tree(GT,v_any).nodes())\n",
    "    OUT = DFS_G - SSC\n",
    "    IN = DFS_GT - SSC\n",
    "    V_rest = set(G.nodes()) - SSC - OUT - IN\n",
    "\n",
    "    TUBES = set()\n",
    "    INTENDRILS = set()\n",
    "    OUTTENDRILS = set()\n",
    "    OTHER = set()\n",
    "\n",
    "    for v in V_rest:\n",
    "        # irv => in reaches node v\n",
    "        irv = len(IN & set(nx.dfs_tree(GT,v).nodes())) is not 0\n",
    "        # vro => node v reaches out\n",
    "        vro = len(OUT & set(nx.dfs_tree(G,v).nodes())) is not 0\n",
    "        if irv and vro:\n",
    "            TUBES.add(v)\n",
    "        elif irv and not vro:\n",
    "            INTENDRILS.add(v)\n",
    "        elif not irv and vro:\n",
    "            OUTTENDRILS.add(v)\n",
    "        elif not irv and not vro:\n",
    "            OTHER.add(v)\n",
    "\n",
    "    FRINGE = set()\n",
    "    DISCONNECTED = set()\n",
    "    for o in OTHER:\n",
    "        # orIT => node o reaches INTENDRILS  \n",
    "        orIT = len(INTENDRILS & set(nx.dfs_tree(G,o))) is not 0\n",
    "        # OTro => OUTTERNDIRLS reaches node o\n",
    "        OTro = len(OUTTENDRILS & set(nx.dfs_tree(GT,o))) is not 0\n",
    "        if orIT or OTro:\n",
    "            FRINGE.add(o)\n",
    "        else:\n",
    "            DISCONNECTED.add(o)\n",
    "    \n",
    "    TENDRILS = INTENDRILS.union(OUTTENDRILS)\n",
    "    \n",
    "    def component_result(name, graph_nodes):\n",
    "        return{ name        : len(graph_nodes),\n",
    "                name + \"_s\" : sum([G.nodes()[node][\"is_sink\"] for node in graph_nodes]),\n",
    "                name + \"_m\" : sum([G.nodes()[node][\"is_miner\"] for node in graph_nodes])}\n",
    "\n",
    "    result_dict = dict()\n",
    "    result_dict.update(component_result(\"nodes\", G.nodes()))\n",
    "    result_dict.update(component_result(\"ssc\", SSC))\n",
    "    result_dict.update(component_result(\"in\", IN))\n",
    "    result_dict.update(component_result(\"out\",OUT))\n",
    "    result_dict.update(component_result(\"tubes\", TUBES))\n",
    "    result_dict.update(component_result(\"tendrils\", TENDRILS))\n",
    "    result_dict.update(component_result(\"fringe\", FRINGE))\n",
    "    result_dict.update(component_result(\"disconnected\", DISCONNECTED))\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_walker(files, json_output,directory):\n",
    "    d = {}\n",
    "    #files = os.listdir(directory)\n",
    "    #files = directory\n",
    "    for file in files:\n",
    "        with open(directory+'/'+file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            G = nx.read_graphml(f)\n",
    "            bowtie_dict = bowtie_analysis(G)\n",
    "            with open(json_output, \"r+\") as fi:\n",
    "                data = json.load(fi)\n",
    "                new_entry = {file[:-8] : bowtie_dict}\n",
    "                data.update(new_entry)\n",
    "                fi.seek(0)\n",
    "                json.dump(data, fi, indent=4)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "proccess added\n",
      "proccess added\n",
      "proccess added\n",
      "proccess added\n",
      "started process: p0\n",
      "started process: p1\n",
      "started process: p2\n",
      "started process: p3\n",
      "joined process: p0\n",
      "joined process: p1\n",
      "joined process: p2\n",
      "joined process: p3\n",
      "--- 42.60962653160095 seconds ---\n"
     ]
    }
   ],
   "source": [
    "nr_cores = multiprocessing.cpu_count()\n",
    "print(nr_cores)\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "core_dict = {}\n",
    "np_files = np.array(os.listdir(\"data/NET-btc-heur_0-week\"))\n",
    "directory = \"data/NET-btc-heur_0-week\"\n",
    "#chunk_lst = np.array_split(np_files, nr_cores)\n",
    "chunk_lst = np_files\n",
    "\n",
    "for i in range(4):\n",
    "    process_name = f\"p{i}\"\n",
    "    file_name = f\"test{i}.json\"\n",
    "\n",
    "    with io.open(os.path.join(file_name), 'w') as db_file:\n",
    "        db_file.write(json.dumps({}))\n",
    "    \n",
    "    \n",
    "    core_dict[process_name] = Process(target = files_walker, args=(([chunk_lst[i]],file_name, directory)))\n",
    "    print(\"proccess added\")\n",
    "\n",
    "for key, item in core_dict.items():\n",
    "    item.start()\n",
    "    print(\"started process: {}\".format(key))\n",
    "\n",
    "for key, item in core_dict.items():\n",
    "    item.join()\n",
    "    print(\"joined process: {}\".format(key))\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict = {}\n",
    "for key, item in core_dict.items():\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['2010-09-06.graphml', '2012-04-23.graphml', '2013-09-16.graphml'],\n",
       "       dtype='<U18'),\n",
       " array(['2012-04-30.graphml', '2009-01-26.graphml', '2012-10-08.graphml'],\n",
       "       dtype='<U18'),\n",
       " array(['2012-01-30.graphml', '2012-06-04.graphml'], dtype='<U18'),\n",
       " array(['2012-12-31.graphml', '2010-10-18.graphml'], dtype='<U18'),\n",
       " array(['2009-05-04.graphml', '2010-04-26.graphml'], dtype='<U18'),\n",
       " array(['2013-04-15.graphml', '2010-02-08.graphml'], dtype='<U18'),\n",
       " array(['2011-10-31.graphml', '2011-10-17.graphml'], dtype='<U18'),\n",
       " array(['2013-07-08.graphml', '2011-11-28.graphml'], dtype='<U18')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_files = np.array(os.listdir(\"data/NET-btc-heur_0-week\"))\n",
    "directory = \"data/NET-btc-heur_0-week\"\n",
    "chunk_lst = np.array_split(np_files, )\n",
    "chunk_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2010-09-06.graphml']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_files = np.array(os.listdir(\"data/NET-btc-heur_0-week\"))\n",
    "directory = \"data/NET-btc-heur_0-week\"\n",
    "#chunk_lst = np.array_split(np_files, nr_cores)\n",
    "chunk_lst = np_files\n",
    "[chunk_lst[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(directory+'/'+chunk_lst[3][3], 'r', encoding='utf8', errors='ignore') as f:\n",
    "    G = nx.read_graphml(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p0'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "process_name = f\"p{i}\"\n",
    "process_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test0.json'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_name = f\"test{i}.json\"\n",
    "process_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
