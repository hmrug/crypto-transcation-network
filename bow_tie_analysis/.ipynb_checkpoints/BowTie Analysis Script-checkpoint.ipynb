{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "from multiprocessing import Process\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm inspired by https://github.com/jeroenvldj/bow-tie_detection/blob/master/bow-tie_detection.py\n",
    "\n",
    "def bowtie_analysis(G):\n",
    "    # reverse all direction of the graph\n",
    "    GT = nx.reverse(G, copy=True)\n",
    "    # calculate SSC\n",
    "    SSC = max(list(nx.strongly_connected_components(G)),key=len)    \n",
    "    \n",
    "    \n",
    "    # take any node n from SSC and do a depth first search \n",
    "    # through directed graph beginning from node n\n",
    "    v_any = list(SSC)[0]\n",
    "    DFS_G = set(nx.dfs_tree(G,v_any).nodes())\n",
    "    DFS_GT = set(nx.dfs_tree(GT,v_any).nodes())\n",
    "    OUT = DFS_G - SSC\n",
    "    IN = DFS_GT - SSC\n",
    "    V_rest = set(G.nodes()) - SSC - OUT - IN\n",
    "\n",
    "    TUBES = set()\n",
    "    INTENDRILS = set()\n",
    "    OUTTENDRILS = set()\n",
    "    OTHER = set()\n",
    "\n",
    "    for v in V_rest:\n",
    "        # irv => in reaches node v\n",
    "        irv = len(IN & set(nx.dfs_tree(GT,v).nodes())) is not 0\n",
    "        # vro => node v reaches out\n",
    "        vro = len(OUT & set(nx.dfs_tree(G,v).nodes())) is not 0\n",
    "        if irv and vro:\n",
    "            TUBES.add(v)\n",
    "        elif irv and not vro:\n",
    "            INTENDRILS.add(v)\n",
    "        elif not irv and vro:\n",
    "            OUTTENDRILS.add(v)\n",
    "        elif not irv and not vro:\n",
    "            OTHER.add(v)\n",
    "\n",
    "    FRINGE = set()\n",
    "    DISCONNECTED = set()\n",
    "    for o in OTHER:\n",
    "        # orIT => node o reaches INTENDRILS  \n",
    "        orIT = len(INTENDRILS & set(nx.dfs_tree(G,o))) is not 0\n",
    "        # OTro => OUTTERNDIRLS reaches node o\n",
    "        OTro = len(OUTTENDRILS & set(nx.dfs_tree(GT,o))) is not 0\n",
    "        if orIT or OTro:\n",
    "            FRINGE.add(o)\n",
    "        else:\n",
    "            DISCONNECTED.add(o)\n",
    "    \n",
    "    TENDRILS = INTENDRILS.union(OUTTENDRILS)\n",
    "    \n",
    "    def component_result(name, graph_nodes):\n",
    "        return{ name        : len(graph_nodes),\n",
    "                name + \"_s\" : sum([G.nodes()[node][\"is_sink\"] for node in graph_nodes]),\n",
    "                name + \"_m\" : sum([G.nodes()[node][\"is_miner\"] for node in graph_nodes])}\n",
    "\n",
    "    result_dict = dict()\n",
    "    result_dict.update(component_result(\"nodes\", G.nodes()))\n",
    "    result_dict.update(component_result(\"ssc\", SSC))\n",
    "    result_dict.update(component_result(\"in\", IN))\n",
    "    result_dict.update(component_result(\"out\",OUT))\n",
    "    result_dict.update(component_result(\"tubes\", TUBES))\n",
    "    result_dict.update(component_result(\"tendrils\", TENDRILS))\n",
    "    result_dict.update(component_result(\"fringe\", FRINGE))\n",
    "    result_dict.update(component_result(\"disconnected\", DISCONNECTED))\n",
    "    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_walker(files, json_output,directory):\n",
    "    #files = os.listdir(directory)\n",
    "    #files = directory\n",
    "    for file in files:\n",
    "        with open(directory+'/'+file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            G = nx.read_graphml(f)\n",
    "            bowtie_dict = bowtie_analysis(G)\n",
    "            with open(json_output, \"r+\") as fi:\n",
    "                data = json.load(fi)\n",
    "                new_entry = {file[:-8] : bowtie_dict}\n",
    "                data.update(new_entry)\n",
    "                fi.seek(0)\n",
    "                json.dump(data, fi, indent=4)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 257.3439128398895 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "nr_cores = multiprocessing.cpu_count()\n",
    "\n",
    "core_dict = {}\n",
    "np_files = np.array(os.listdir(\"data/NET-btc-heur_0-week\"))\n",
    "directory = \"data/NET-btc-heur_0-week\"\n",
    "chunk_lst = np.array_split(np_files, nr_cores)\n",
    "\n",
    "for i in range(nr_cores):\n",
    "    process_name = f\"p{i}\"\n",
    "    file_name = f\"test{i}.json\"\n",
    "\n",
    "    with io.open(os.path.join(file_name), 'w') as db_file:\n",
    "        db_file.write(json.dumps({}))\n",
    "\n",
    "    core_dict[process_name] = Process(target = files_walker(list(chunk_lst[i]),file_name, directory))\n",
    "    \n",
    "    core_dict[process_name].start()\n",
    "\n",
    "    #core_dict[process_name].join()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
